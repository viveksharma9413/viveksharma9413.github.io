<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>6sense - Detailed Experience | Vivek Sharma</title>
  <link rel="stylesheet" href="../assets/css/style.css" />
  <link rel="stylesheet" href="../assets/css/timeline.css" />
  <link rel="icon" href="../assets/images/favicon.ico" />
  <style>
    body {
      background-color: #f8fafc;
      padding: 2rem;
      font-family: 'Segoe UI', sans-serif;
    }

    .modal-card {
      max-width: 1000px;
      margin: 0 auto;
      background: #fff;
      border-radius: 12px;
      box-shadow: 0 10px 30px rgba(0,0,0,0.1);
      padding: 2rem;
      position: relative;
    }

    .modal-card h2 {
      color: #2563eb;
      font-size: 2rem;
      margin-bottom: 1rem;
    }

    .modal-card h3 {
      color: #1e40af;
      font-size: 1.3rem;
      margin-top: 2rem;
    }

    .modal-card table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 1rem;
    }

    .modal-card table th, .modal-card table td {
      border: 1px solid #e2e8f0;
      padding: 0.75rem;
      text-align: left;
    }

    .modal-card table th {
      background-color: #eff6ff;
      color: #1e40af;
      font-weight: bold;
    }

    .pill {
      background-color: #dbeafe;
      color: #1e3a8a;
      padding: 0.4rem 0.8rem;
      border-radius: 6px;
      font-weight: 500;
      font-size: 0.95rem;
      display: inline-block;
      margin: 0.25rem;
    }

    .section-divider {
      border-top: 1px dashed #cbd5e1;
      margin: 2rem 0;
    }
  </style>
</head>
<body>
  <div class="modal-card">

    <h2>üè¢ 6sense</h2>
    <p><strong>Senior Data Engineer</strong></p>
    <p>üìç Oct 2022 ‚Äì Sept 2024 | Bengaluru, India (Remote/Hybrid)</p>

    <div class="section-divider"></div>

    <h3>Professional Summary</h3>
    <p>
      At <strong>6sense</strong>, I worked in the Big Data Platform team, helping build frameworks for real-time and batch ingestion,
      reporting, and analytics. I supported platform-wide initiatives powering one of the industry‚Äôs largest Spark + Trino clusters ‚Äî
      handling over 1 million jobs per day.
    </p>
    <p>
      I contributed to customer-facing product pipelines, led internal tooling efforts, and optimized workflows across data engineering,
      marketing ops, and analytics stakeholders.
    </p>

    <div class="section-divider"></div>

    <h3>Core Competencies</h3>
    <div>
      <span class="pill">Spark / Delta Lake / Hive / Trino</span>
      <span class="pill">PySpark, SQL, Python</span>
      <span class="pill">Singlestore, MySQL, PostgreSQL</span>
      <span class="pill">ETL Frameworks</span>
      <span class="pill">S3, HDFS, AWS Lambda</span>
      <span class="pill">CDC & Archival Pipelines</span>
      <span class="pill">Cross-region Deployments</span>
    </div>

    <div class="section-divider"></div>

    <h3>Team & Collaboration</h3>
    <ul>
      <li>Worked closely with Staff Engineers on CDC strategy, Spark tuning, and performance benchmarks</li>
      <li>Owned multiple internal tools used across the Big Data team</li>
      <li>Interfaced with Marketing Ops, SalesOps, and Data Science teams for delivery and quality</li>
      <li>Supported multi-region replication and schema contract design</li>
      <li>Contributed to platform-wide AWS cost optimizations and data freshness SLAs</li>
    </ul>

    <div class="section-divider"></div>

    <h3>Key Projects</h3>
    <table>
      <thead>
        <tr>
          <th>Project</th>
          <th>Role</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Singlestore ETL Framework</td>
          <td>Lead Developer</td>
          <td>Built a unified framework for ingesting batch + CDC data into Singlestore with support for Trino, Hive, and auto-scaling ingestion patterns.</td>
        </tr>
        <tr>
          <td>Data Extractor Utility</td>
          <td>End-to-End Owner</td>
          <td>Generic utility to archive filtered data from MySQL/Postgres into S3/HDFS. Enabled retention controls and self-serve ingestion.</td>
        </tr>
        <tr>
          <td>Hive Table Cloning Utility</td>
          <td>Lead Developer</td>
          <td>Tool to replicate production Hive tables into dev environments. Cut debugging time drastically for analysts and data engineers.</td>
        </tr>
        <tr>
          <td>Delta & Spark Batch Optimization</td>
          <td>Contributor</td>
          <td>Optimized TB-scale Spark jobs by tuning shuffle partitioning, caching logic, and file layout strategies.</td>
        </tr>
        <tr>
          <td>Cross-Team Schema Ownership</td>
          <td>Collaborator</td>
          <td>Maintained contracts and schema evolution strategies across data sources to prevent pipeline breakage and reduce rework.</td>
        </tr>
      </tbody>
    </table>

    <div class="section-divider"></div>

    <h3>Key Impact Metrics</h3>
    <table>
      <thead>
        <tr><th>Area</th><th>Impact</th></tr>
      </thead>
      <tbody>
        <tr><td>Platform Efficiency</td><td>‚è± Improved batch + CDC ingestion latency across multiple Spark pipelines</td></tr>
        <tr><td>Developer Productivity</td><td>üöÄ Enabled faster debugging and staging through table cloning tools</td></tr>
        <tr><td>Data Consistency</td><td>üîÑ Standardized schema contracts across product and analytics teams</td></tr>
        <tr><td>Cost Optimization</td><td>üí∞ Supported S3 + HDFS archival strategy to reduce storage and read costs</td></tr>
        <tr><td>Tooling Adoption</td><td>‚öôÔ∏è Internal utilities adopted across Big Data and Customer Intelligence teams</td></tr>
      </tbody>
    </table>

  </div>
</body>
</html>